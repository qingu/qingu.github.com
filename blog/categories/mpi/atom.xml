<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: MPI | MetMan]]></title>
  <link href="http://qingu.github.com/blog/categories/mpi/atom.xml" rel="self"/>
  <link href="http://qingu.github.com/"/>
  <updated>2014-04-21T06:55:01+08:00</updated>
  <id>http://qingu.github.com/</id>
  <author>
    <name><![CDATA[Qingu Jiang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[MPI性能分析库-libmpitrace]]></title>
    <link href="http://qingu.github.com/blog/2013/11/02/mpixing-neng-fen-xi-ku-libmpitrace/"/>
    <updated>2013-11-02T18:58:00+08:00</updated>
    <id>http://qingu.github.com/blog/2013/11/02/mpixing-neng-fen-xi-ku-libmpitrace</id>
    <content type="html"><![CDATA[<h3 id="section">前言</h3>

<p>MPI性能分析库libmpitrace可用来分析程序中MPI函数调用所花时间并能跟踪MPI函数调
用情况。工作原理是程序链接libmpitrace库时，该库将拦截程序中MPI调用，使用封装了性
能分析功能的MPI函数，从而获得性能分析和跟踪信息。该库同时提供一组函数接口用于
定制收集、跟踪数据的方式。注意该库只能用在单线程应用或MPI函数只用一个线程执行
的应用中。</p>

<!--more-->

<h3 id="libmpitrace">编译链接libmpitrace</h3>

<ul>
  <li>编译时需要加上<code>-g</code>选项
    <ul>
      <li>加<code>-g</code>编译选项可保证获得映射回程序源代码的性能信息</li>
      <li>可能要关掉或降低应用优化等级（-O2,-O1,…）</li>
      <li>高级优化会影响调试信息的准确性，并影响调用堆栈行为</li>
    </ul>
  </li>
  <li>链接库选项加上<code>-L/path/to/libmpitrace -lmpitrace</code>
    <ul>
      <li>-lmpitrace必须加在 -lmpich之前，从而保证调用的MPI函数是封装过的MPI函数</li>
    </ul>
  </li>
</ul>

<h3 id="section-1">性能分析数据输出</h3>

<p>加上以上选项后，编译好后正常运行程序，会生成<code>mpi_profile.rank</code>文件（
rank为进程号）。可以看下mpi_profile.0文件里内容（#后为我添加的说明,原数据并不
存在这些内容）：</p>

<p>```
#数据来自哪个进程
Data for MPI rank 0 of 1024:
#数据统计区域，无额外控制时默认为 MPI_Init()和MPI_Finalize()之间代码区域
Times and statistics from MPI_Init() to MPI_Finalize().
#各个MPI函数调用次数，平均传输数据量bytes以及所花时间统计
—————————————————————–
MPI Routine                  #calls     avg. bytes      time(sec)
—————————————————————–
MPI_Send                        144         7448.0          0.002
MPI_Recv                        144         7448.0          0.018
MPI_Sendrecv                    432         8157.3          0.102
MPI_Allreduce                   285           31.7          0.520
—————————————————————–
#总的通信时间、代码区域总的墙钟时间、用户cpu时间、系统时间、最大内存使用
total communication time = 0.642 seconds.
total elapsed time       = 1.397 seconds.
user cpu time            = 1.137 seconds.
system time              = 0.000 seconds.
maximum memory size      = 1423168 KBytes.</p>

<hr />
<p>Message size distributions:</p>

<p>MPI_Send                  #calls    avg. bytes      time(sec)
                             144        7448.0          0.002</p>

<p>MPI_Recv                  #calls    avg. bytes      time(sec)
                             144        7448.0          0.018</p>

<p>MPI_Sendrecv              #calls    avg. bytes      time(sec)
                             144        3192.0          0.004
                             144        6384.0          0.036
                             144       14896.0          0.061</p>

<p>MPI_Allreduce             #calls    avg. bytes      time(sec)
                               1           8.0          0.001
                             158          16.0          0.303
                              32          28.0          0.048
                              64          52.0          0.117
                              30          76.0          0.051</p>

<hr />
<p>#通信最少、中间、最大时间以及来自哪个进程
Communication summary for all tasks:</p>

<p>minimum communication time = 0.105 sec for task 52
  median  communication time = 0.592 sec for task 940
  maximum communication time = 0.804 sec for task 25</p>

<h1 id="section-2">根据通信时间对所有进程排序</h1>
<p>MPI tasks sorted by communication time:
taskid      comm(s)  elapsed(s)     user(s)     size(KB)   switches
    52        0.11        1.40        0.70       478620           3
    41        0.11        1.40        0.70       477644          10
    59        0.11        1.40        0.70       477064           8
    46        0.11        1.40        0.69       477040           9
…
```</p>

<p>默认输出所有进程的MPI通信情况（这一点和我看到的资料情况不太相同，资料上都说
缺省只输出4个文件）。可以通过设置环境变量<code>SAVE_ALL_TASKS=no</code>为0和MPI
通信时间最少、最多、中间值四个文件（如果进程0通信时间最少、最多、中间值，则为
三个文件）。</p>

<h3 id="section-3">控制需要性能分析的代码区域</h3>

<p>如果不额外设置，默认收集MPI_Init()和MPI_Finalize()之间的计时数据。可以从
mpi_profile.rank中第二行数据看出来<code>Times and statistics from
MPI_Init() to MPI_Finalize().</code></p>

<p>如果关注指定代码区的MPI通信情况，可以在指定代码区前后插入<code>summary_start()</code>和
<code>summary_stop()</code>子程序。</p>

<p><code>
call summary_start()
call do_work()
call summary_stop()
</code></p>

<p>可以从生成的mpi_profile.rank中看到统计区域变成
<code>Times and statistics from summary_start() to summary_stop().</code></p>

<p><strong><code>summary_start()</code>和<code>summary_stop()</code>调用可用在循环结构中，得到的数据是指定代
码区数据的总和</strong></p>

<p>GRAPES中需要统计某一积分步内MPI通信情况，可以用以下代码：</p>

<p><code>
if(step == n)then
  call summary_start()
endif
call model_subroutine()
if(step == n)then
  call summary_stop()
endif
</code></p>

<h3 id="section-4">结束语</h3>

<p>关于libmpitrace库中跟踪（trace）功能还不太懂，感觉是通过生成的trace数据中地址回溯到
源代码行，目前不需要该功能，以后再说吧。</p>

<p><strong>参考文章</strong></p>

<ol>
  <li>
    <p><a href="http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/index.jsp?topic=%2Fcom.ibm.cluster.pedev.v1r2.pedev100.doc%2Fbl7ug_compilelinklibmpitrace.htm">http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/index.jsp?topic=%2Fcom.ibm.cluster.pedev.v1r2.pedev100.doc%2Fbl7ug_compilelinklibmpitrace.htm</a></p>
  </li>
  <li>
    <p>Bob Walkup,Internal Use MPI Wrappers for BGQ.</p>
  </li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GRAPES源码分析之计时系统]]></title>
    <link href="http://qingu.github.com/blog/2013/04/23/grapesyuan-ma-fen-xi-zhi-ji-shi-xi-tong/"/>
    <updated>2013-04-23T16:14:00+08:00</updated>
    <id>http://qingu.github.com/blog/2013/04/23/grapesyuan-ma-fen-xi-zhi-ji-shi-xi-tong</id>
    <content type="html"><![CDATA[<h3 id="section">前言</h3>

<p>模式开发者（尤其是并行计算开发者）对模式各部分执行花费的时间很感兴趣，需要了解哪里耗时比较长，进而对其进行优化。普通用户一般不需要这个功能。计时系统的一般思路：在需要计时部分的起点和终点设置计时器（调用计时子程序），两个点时间差即为计时部分所耗时间。</p>

<!--more-->

<h3 id="grapes">GRAPES计时系统</h3>

<p>目前发现GRAPES中有3套计时系统，分别在 <code>module_integrate</code> 、 <code>sovle_grapes</code> 及 <code>ritche_puw_jin</code> 中，对各自调用的子程序进行计时。计时子程序使用MPI墙钟函数。为了使版本统一，使用条件预编译方法控制开启实现计时功能（MPI同步计时会使模式整体运行时间过长，不适用于业务）。</p>

<h4 id="solvegrapes">solve_grapes计时系统</h4>

<p>以下以solve_grapes中计时系统进行说明</p>

<ul>
  <li>计时开始部分（别忘了引入mpi模块， <code>use MPI</code> ）</li>
</ul>

<p>```
	#ifdef DETAIL_TIMING
	   integer :: ierr, comm
	   real<em>8 :: start_time, end_time
	   real</em>8 ,save::  wtimer(80,2)=0. 
	   !保存计时数据的二维数组，第一维指不同计时部分，第二维指不同步和同步两种情况
	#endif</p>

<pre><code>#ifdef DETAIL_TIMING
   comm=mpi_comm_world          
   call mpi_barrier(comm, ierr)  !各进程同步，使各进程计时起点一致
   start_time=mpi_wtime()        !记录起点墙钟时间
   call get_communicator(comm)
#endif ```
</code></pre>

<ul>
  <li>第一个计时部分终点处</li>
</ul>

<p><code>
	#ifdef DETAIL_TIMING
	     end_time=mpi_wtime()             ！本进程计时终点墙钟时间
	     wtimer(1,1)=end_time-start_time  ! 未同步情况本进程该计时部分所用时间
	     call mpi_barrier(comm, ierr)     ! 同步
	     end_time=mpi_wtime()             ！所有进程都执行完该部分时墙钟时间
	     wtimer(1,2)=end_time-start_time  ！同步情况下执行时间
	     start_time=end_time              ! 下一计时部分的起点墙钟时间
	#endif
</code></p>

<ul>
  <li>
    <p>以后每一计时部分计时思路与上述相同</p>
  </li>
  <li>
    <p>在 <code>solve_grapes</code> 最后输出每步积分步各调用部分执行时间</p>
  </li>
</ul>

<p><code>
	#ifdef DETAIL_TIMING
	     write(73,*) 'step',step,':'
	     write(73,*) "no_barrier for timing *************"
	     write(73,'(10F8.4)') (wtimer(i,1),i=1,80)
	     write(73,*) "barrier for timing *************"
	     write(73,'(10F8.4)') (wtimer(i,2),i=1,80)
	#endif
</code>
文件号为73的文件是在 <code>module_integrate</code> 中定义的。</p>

<p><code>
   write(filename,'(A7, I5.5)') 'Timing.',myprcid  !各进程计时输出文件名，
   open (73,file=filename,form='formatted')        !有多少进程，就有多少文件
</code></p>

<h4 id="section-1">其它</h4>

<p><code>module_integrate</code> 中计时系统与 <code>solve_grapes</code> 中一样，只不过计时都为非同步情况下的时间，保留在 <code>wtimer_integrate</code> 一维数组中。预编译条件与 <code>solve_grapes</code> 相同，都为 <code>DETAIL_TIMING</code> 。</p>

<p><strong>注：每一积分步 <code>solve_grapes</code> 执行时间保留在 <code>wtimer_integrate(5)</code> 中.</strong></p>

<p>还有一套是在 <code>module_semi_lag</code> 中 <code>ritche_puw_jin</code> 中定义的，预编译条件为 <code>SL_TIMING</code> 。</p>

<p>如果需要开启计时系统，需在 <code>configure.grpaes</code> 中预处理器选项 <code>ARCHFLAGS</code> 中加上类似 <code>-D predefineName</code> ，如 <code>-DSL_TIMING</code> 。</p>
]]></content>
  </entry>
  
</feed>
